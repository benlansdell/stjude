<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>The neuronal credit assignment problem as causal inference</title>

		<link rel="stylesheet" href="css/reset.css">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/black.css">
		<link rel="stylesheet" href="cust_black.css" id="theme">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/monokai.css">

		<!-- Mathjax for math typesetting -->
		<script type="text/x-mathjax-config">
			MathJax.Hub.Config({TeX: {extensions: ["color.js"]}});
		</script>
		
		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">

				<section data-background="assets/brain2_darkest.png">
					<h1>The neuronal credit assignment problem as causal inference</h1>
				<hr>
				<p style="text-align: center; font-size: larger; text-shadow: 0px 0px 0px #0000ff;">Ben Lansdell, Bioengineering UPenn<br><br>St Jude Children's Research Hospital<br>July 22nd 2020
				<aside class="notes">
				  <span style="color: red">
				  </span> •		  

				  Ok, thank you for the invite:

				  It's nice to have the chance to present something about my research,
				  even if remotely.

				  I'm going to talk today about my work during my postdoc at here Penn, I'll have just 
				  time to give just an overview, and to describe one project in some detail.

				  In the end I'll describe a couple of other projects I've spent time on 
				  that may be of interest to people here.

				  So, my postdoctoral work has focused largely on thinking about causality in 
				  neuroscience and machine learning. 

				  So I'll start just by giving an overview of why I think that's an interesting area. 

				  Background photo credit: 
				  Confocal micrograph of hippocampal interneurons that express serotonin receptors (green) co-labeled in red for calretinin, and counterstained with DAPI (blue), a marker of cell nuclei. Credit: Margaret I. Davis


				  • <span style="color: green"></span>
				</aside>
				  </section>
		  
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- The overall introduction -->
		  <!-- ---------------------------------------------------------------------- -->

	  <section data-background-color="#000000">
		<h2>Causality in neuroscience</h2>
		<div>
		<img src="assets/zebrafish.png" style="border: 0px" width="70%">
		</div>
		<p class="rcred">Fosque et al, Science 2015<br>
		Thorsen et al, J Experimental Biology 2004		
		</p>
		<div style="clear: both">
		</div>
		<aside class="notes">
		<span style="color: red"></span> •
		-------------- 

		Arguably, most scientific theories demand causal explanations. We want to know what mechanism is behind some behavior or phenomenon of interest. So in neuroscience, we want to know how patterns of neuronal activity produces a behavior, such as swimming, as in this image here with a zebrafish larvae. What it really means for some mechanism to be responsible for some behavior of interest is that we have an understanding of, when we tweak some part of the underlying system, we can predict what the effect will be on the behavior of interest. The better we can do this, the more robust our understanding. This, almost by definition, involves understanding the causal relationships at play.
		
		Causal relationships are hard won in biology, and neuroscience is definitely no exception. 

		It is hard to establish how one brain region, or even how one neuron, has a causal effect on some behavior, or 
		some other neuron or brain region. This is really challenging for a number of reasons:

		* complicated feedback dynamics (with environment, and within the nervous system)
		* hard to control 
		* hard to fully observe
		* high dimensional 
	   
		All of these things make determining causal relationships difficult
		
		• <span style="color: green"></span>
	  </aside>	</section>

<!-- Slide about causality-->


  <section data-background-color="#000000">
	<h2>Causality in neuroscience</h2>
	Challenges to obtaining causal relationships in neuroscience:
	<div>
	<div id = "left">
		<br>
		<p style="text-align: left"></p>
		<ul>
			<li> complicated feedback dynamics 
			<li> hard to control (make targeted interventions)
			<li> rarely can fully observe (confounded)
			<li> high dimensional (hard to develop intuition)
		</ul>
	</div>
	<div id = "right">
		<br>
		<img src="assets/zebrafish2.png" style="border: 0px" width="65%">
	</div>
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
	In neuroscience, we generally seek a mechanistic, or causal model, of a system. Yet, only recently have we begun to have good enough experimental tools to do that. Even with current methods, it's generally the case that neural data is hugely undersample, which can introduce confounding, and this makes intepreting relationships in neural data difficult. 

	More generally, when we think about the tasks that neuronal networks perform, some of the same issues relate: how do we learn transferable models of the world, and how does this relate to learning causal relationships between our own actions and the world, and between other objects in the world?
	• <span style="color: green"></span>
  </aside>	</section>

  <section data-background-color="#000000">
	<h2>(Causality in machine learning)</h2>
	Causation also relates to a number of challenges in machine learning: 
	<div>
	<div id = "left">
		<br>
		<img src="assets/chocolate.png" width="93%">
		<p class="rcred">Messerli et al, N Engl J Med 2012</p>
	</div>
	<div id = "right">
		<br>
		<p style="text-align: left"></p>
		<ul>
			<li>Causal models are more robust to changes in environment/distribution: better transfer, generalization</li>
			<li>Fairness: strong associations are not causal, and may be unfair/biased/prejudiced</li>
			<li>Safety: observational data may not say what happens when we act/intervene/change distributions</li>
		</ul>
	</div>
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
		I won't touch on this much today, but just to describe the other side of my research, it is the case that causation also relates to a number of challenges in machine learning today. 

		It's fair to say that a lot of recent advances in machine learning stem from building better predictive models. They find correlations in data, but are in general blind to the causal structure underlying such relations. For instance, there's many examples I could have chosen for this plot on the left, but if, we plot the number of nobel prizes awarded for a country, per capita vs the chocolate consumption per capita, we observe a strong positive relationship. But this of course doesn't tell us there is a causal relationship. If we were to go and make interventions, and make some country consume more chocolate, that of course doesn't mean we would expect to see an increase in Nobel prizes. Such models can be completely ignorant of the causal processes underlying.

		This relates to challenges in machine learning right now:
		* some ML models can take millions of datapoints to work well, this is in part because such models are very specific to a particular problem.  model trained on one problem does not generalize to others very well. Causal models, in contrast, are more robust to changes in the environment. This means that we should expect learning causal models to be more generalizable to other environments, distributions, etc, and this means it doesn't have to start from scratch in every new task you're interested in -- causal models can be more data efficient
		* It relates to issues of fairness. Strong associations need not be causal, and making policy decisions based on associations may be biased, prejudiced, or unfair. 
		* And it relates to safety. Another way to phrase this is that observational data may not say what happens when we change the system, and thus there are no safey guarantees about what happens when you do change things. 

		These are some of the key challenges facing machine learning today. 
	• <span style="color: green"></span>
  </aside>	</section>

  <section data-background-color="#000000">
	<h2>Neuroscience, machine learning and causality</h2>
	<div>
	<div id = "left">
		<br>
		<img src="assets/chocolate.png" width="93%">
	</div>
	<div id = "right">
		<br>
		<img src="assets/zebrafish2.png" style="border: 0px" width="45%">
	</div>
	</div>
	<div style="clear: both">
		<br>
		Claim: progress in both neuroscience and machine learning can come from explicitly casting problems as causal learning problems
	</div>
	<aside class="notes">
	<span style="color: red"></span> •
		* So the idea I have been interested in is that: progress in both neuro and ML can come from explicitly casting problems as causal learning problems.  
	• <span style="color: green"></span>
  </aside>	</section>


  		  <!-- --------------------------------------------- -->
		  <!-- Credit assignment problem as causal inference -->
		  <!-- --------------------------------------------- -->

		  <section data-background="assets/brain2_transformed.png">
			<div>
			</div>
			<aside class="notes">
			<span style="color: red"></span> •

			So we'll see how this plays out in this credit assignment problem. This problem arises when we think about models of learning in the brain. So, we have a neuron embedded in some network, learning some task, and we want to know what are learning models that provide an account of how we learn things efficiently. 
			• <span style="color: green"></span>
		  </aside>	</section>

		  <section data-background-color="#ffffff">
			<h2>Learning in the brain</h2>
			<div>
			<div>
			  <img src="assets/losslandscape.svg" width="50%">
			</div>
		  </div>

		  Find parameters $w$ that minimize a loss/maximize a reward function, $R$

		  <aside class="notes">
			<span style="color: red"></span> •
			
			We can formalize this as follows. The idea is we have a network characterized by some state, some set of synaptic weight parameters w. Different weights will correspond to different responses given some stimulus. We assume that there is some measure of performance, and the process of learning is one of finding parameters w that optimize this performance. We will also say minimize a loss function, or maximize some reward function. For the purposes of this talk, we can think of these things as interchangable. 

			• <span style="color: green"></span>
		  </aside>	</section>

			  <section data-background-color="#ffffff">
				  <h2>Learning in the brain</h2>
				  <div>
				  <div style="text-size: 40pt">
					<br><br><br><br><br>
					<img src="assets/plasticity2.gif" width="60%">
					<br><br><br><br><br>
				  </div>
				</div>
				What are the synaptic update rules used by neurons that provide efficient and flexible learning?
				<p>Must be:
				<ul>
					<li>consistent with known neurophysiology</li>
					<li>good enough at learning complicated tasks</li>
				</ul>
				</p>
				<aside class="notes">
				  <span style="color: red"></span> •
				  So the question is, what are the synaptic update rules, f, that provide efficient and flexible learning? 
				  • <span style="color: green"></span>
				</aside>	</section>
		  
			  <section data-background-color="#ffffff">
				<h2>The neuronal credit assignment problem</h2>
				<p>To learn, a neuron must know its effect on the reward function</p><br>
				<img src="assets/spiketrain.svg" width="80%">
				<p><br>In spiking neural networks, this means something like:</p>
				<ul>
					<li> If, for a given input, a spike <b>increases</b> the reward, the weights leading to that spike should <b>increase</b>
					<li> If, for a given input, a spike <b>decreases</b> the reward, the weights leading to that spike should <b>decrease</b>
				</ul>
				<aside>
					<span style="color: red"></span> •
					If we think about what basic properties such a rule must satisfy. The rule must in some way incorporate the effect the neuron is having on the network's output. 
					[explain figure]
					How a neuron measures this effect is known as the credit assignment problem. 
					• <span style="color: green"></span>
				</aside></section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
			  <section data-background-color="#ffffff">
				<h2>The problem: noise correlations and confounding</h2>
				  <img src="assets/correlation_a.png" width="80%">
				  <p class="fragment" style="text-align: center">$\Rightarrow$ Viewing learning as a <em>causal inference</em> problem may provide insight</p>
				  <aside class="notes">
				  <span style="color: red"></span> •
					So we can think about the first of these problem as follows. If we take a neuron's perspective, neuron 1's. Then in general it is the case that, even for a fixed stimulus, there are correlations between neuron 1 and a neighboring neuron, neuron 2. These correlations can act as confounders. Say neuron 1 observes a negative relationship between its activity and a reward signal. Just based on observing reward, and even neuron 2's activity, it cannot determine if it has any direct effect on the reward signal? These two causal graphs produce the same joint probability distribution. 
					
					If a neuron want to change its synaptic weights in this setting, how should it go about it? 

					Viewing learning as a causal inference problem can provide insight. 

				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				  <h2>Causality</h2>
				  <img src="assets/correlation_a.png" width="80%">
				  <ul>
					<li> Defined in terms of counterfactuals or interventions
					<li> The causal effect: $\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0)$
					<li> How can we predict the causal effect from observation?
				</ul>	    
			  <aside class="notes">
				  <span style="color: red"></span> •
					To keep going I need to make the idea of a causal relationship a bit more precise. What do I mean by a causal relationship? This is, in stats at least, normally defined in terms of interventions of counter factuals. By intervention we mean something that effects the value of one variable. In essence, we force a given variable to take a given value, and erase any relationship it may have previously had with its potential causes. 

					So here, we force neuron 1 to take on a given value, regardless of what neuron 2 is doing. We can see this actually allows us to now distinguish between the two graphs. We can say through interventions if neuron 1 has a direct effect on reward or not. 

					Often we're itnerested in a quantity called the causal effect. E.g. if this is a drug trial, this is expected outcome given treatment or control conditions. 

					A key question in the field is, can we determine the causal effect from observational data (that is, without actually doing the experiments to intervene)?

					If you follow these two slides you get the basis of Pearl's framework for causal inference. And two important things: observational data may not be enough to tell two causal graphs apart, but interventions do make them identifiable. 

				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Causality</h2>
				<img src="assets/intervention.svg" width="80%">
				<ul>
				  <li> Defined in terms of counterfactuals or interventions
				  <li> The causal effect: $\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0)$
				  <li> How can we predict the causal effect from observation?
			  </ul>	    
			<aside class="notes">
				<span style="color: red"></span> •
				• <span style="color: green"></span>
			  </aside>
			</section>
		  
			  <section data-background-color="#ffffff">
				  <h2>Credit assignment as causal inference</h2>
				  <p>What is a neuron's causal effect on reward, and so how should it change to improve performance?
						  $$
						  \beta_i = \mathbb{E}(R|  H_i \leftarrow 1) - \mathbb{E}(R| H_i \leftarrow 0)
						  $$
						</p>
				  <p class="fragment">$\Rightarrow$ How can a neuron perform causal inference?</p>
				  <aside class="notes">
				  <span style="color: red"></span> •
				  So, given these ideas we can rephrase our question about credit assignment again. We can rephase our problem into a network wanting to know which actions cause reward. And, what is a neuron's causal effect on reward, and so how should it change to improve? 

				  Given this interpretation, we can see that methods like those based on REINFORCE address this problem by adding randomization to a neuron's output. That is, you can think of this as a type of RCT approach to doing this causal inference. 

				  The question is, can a neuron solve this causal inference problem without randomizing? 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Credit assignment as causal inference</h2>
				<p>One solution: Randomization</p>

				<p>If independent (unconfounded) noise is added to the system, this can be correlated with reward for an estimate of its reward gradient</p>

				<p>In fact, the REINFORCE algorithm correlates reward with independent pertubations in activity, $\xi^i$:
					$$
					\mathbb{E}( R\xi^i ) \approx \sigma^2 \frac{\delta R}{\delta h^i}
					$$</p>
				<div class="fragment">
				<p>But:</p>
					<ul>
					<li> Requires each neuron measures an IID noise source, $\xi^i$, or knows its output relative to some expected output
					<li> Only well characterized in specific circuits e.g. birdsong learning (Fiete and Seung 2007)</li>
					</ul>
				</div>
				<aside class="notes">
				<span style="color: red"></span> •
				So, given these ideas we can rephrase our question about credit assignment again. We can rephase our problem into a network wanting to know which actions cause reward. And, what is a neuron's causal effect on reward, and so how should it change to improve? 

				Given this interpretation, we can see that methods like those based on REINFORCE address this problem by adding randomization to a neuron's output. That is, you can think of this as a type of RCT approach to doing this causal inference. 

				The question is, can a neuron solve this causal inference problem without randomizing? 
				• <span style="color: green"></span>
			  </aside>
			</section>


			  <section data-background-color="#ffffff">
				  <h2>Causal learning without randomization</h2>
				  An observation: decisions made with arbitary thresholds let us observe counterfactuals
				  <br>
				  <img src="assets/rdd.svg" width="50%">
				  <p class="rcred">Adapted from Moscoe et al, J Clin Epid 2015</p>
					Known as regression discontinuity design (RDD) in economics
				<aside class="notes">
				  <span style="color: red"></span> •
				  We make use of the following observation. Decisions made with arbitary thresholds let us estiamte causal effects. The marginal populations only differ by the fact that one groups recieves the treatment, and the other does not. Thus this removes confounds and lets us measure beta. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color="#ffffff">
				<h2>Two more observations:</h2>
					<ol>
						<li> A neuron <em>only</em> spikes if its input is above a threshold
						<li> A spike can have a measurable effect on outcome and reward
					</ol>
		  
					<p class="fragment">Suggests regression discontinuity design can be used by a neuron to estimate its causal effect.<br>		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  This is useful in the neural learning setting. Because a neuron spikes when its input is above a threshold. In some cases at least, a spike does have a measureable effect on a reward signal. 
		  
				  This suggests a neuron could use RDD to estimate its causal effect.

				  Specifically, we will investigate the question, can a neuron use RDD to estimate beta, defined as follows?
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
			  <section data-background-color='#ffffff'>
				<h2>RDD for solving credit assignment</h2>
				  <img src="assets/rdd_fig1a.svg" width="55%">
				  <p class="rcred">Lansdell and Kording, bioRxiv 2019</p>	
					<ul>
						<li> Inputs that place the neuron close to threshold are unbiased estimate of causal effect
						<li> Estimate piece-wise constant model: $$R = \gamma_i + \beta_i H_i$$
					</ul>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So, over a fixed time window, a neuron should only use inputs where it was within p of threshold to learn from. Using this data, we propose a neuron can estimate a piecewise linear model which it can use to infer beta. By design, this works with correlated noise sources -- it only needs to observe the reward and how close it was to spiking, it does not need to identify an IID noise source to estimate causal effects. 
				  • <span style="color: green"></span>
				</aside>
			  </section>

			  <!-- Take a step back and talk about the fact that spiking is a feature, not a bug, here?? -->

			  <section data-background-color='#ffffff'>
				  <h2>A small demonstration</h2>
				  <p>The two-neuron network with noise correlations</p>
				  <img src="assets/fig2a_lif.svg" width="35%">
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  		  
			  <section data-background-color='#ffffff'>
				<h2>A small demonstration</h2>
				<p>Can use RDD to estimate the causal effect</p>
				<img src="assets/fig2a_ace.svg" width="45%">
		
			  <aside class="notes">
				<span style="color: red"></span> •
				So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				• <span style="color: green"></span>
			  </aside>
			</section>

			<section data-background-color='#ffffff'>
				<h2>A small demonstration</h2>
				<p>Works in cases where a correlational estimator fails</p>
				$$\beta = \mathbb{E}(R|H\leftarrow 1) - \mathbb{E}(R|H\leftarrow 0), \quad \beta_{OD} = \mathbb{E}(R|H=1) - \mathbb{E}(R|H=0)$$
				<img src="assets/fig2a_lossland.svg" width="85%">
		
			  <aside class="notes">
				<span style="color: red"></span> •
				So how does this look in simulation? We demonstrate the idea on a simple 2 neuron network. We inject two neurons with correlated noise, xi. They have what are called leaky integrate and fire dynamics, which includes a spiking mechanism when the voltage exceeds some value theta. Each neuron runs RDD for a window size p. We can observe, if we fit a piecewise constant model the estimate becomes unbiased for small p. If we use the piece wise linear model, the estimate is in fact unbiased for large value of also. We can compare the RDD estimator to what we called the observed dependence, which takes p = 1. that is we just use all points below and above the threhsold to estimate the relationship. This is confounded for highly correlated inputs. We see on these graphs that bias. 
				• <span style="color: green"></span>
			  </aside>
			</section>
			
			  <section data-background-color='#ffffff'>
				  <h2>A small demonstration</h2>
				  <p>Under some assumptions</p>
				$$
				\frac{\partial R}{\partial w^i_j} \approx \frac{\partial H^i}{\partial w^i_j} \beta^i
				$$
					<ul>
					<li> Can relate causal effect to gradients $\Rightarrow$ derive stochastic gradient descent learning rule 
				  	</ul><br>
				  <img src="assets/fig4.svg" width="100%">
				  <ul>
					<li> Learning trajectories are less biased and converge faster
				  	</ul>
				<aside class="notes">
				  <span style="color: red"></span> •
				  The rule takes the following form. 
		  
				  We observe the trajectories are less biased and converge faster than when using the observed dependence.
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			<section data-background-color='#ffffff'>
				<h2>A larger example</h2>
				<div>
					<img src="assets/fig_7.png" width="60%">
					<!-- Scaling with network size and depth-->
			</div>
		
			  <aside class="notes">
				<span style="color: red"></span> •
		
				• <span style="color: green"></span>
			  </aside>
			</section>

			  <!-- <section data-background-color='#ffffff'>
				  <h2>Application to brain-computer interface learning</h2>
				  <div>
					  <ul>
						  <li> In single-unit BCIs, individual neurons are trained through biofeedback </li>
						  <li>Here, causal effect of a neuron is known <em>by construction</em> </li> 
						  <li> How does the network change specifically the control neuron's activity? 
						  <li> Must solve causal inference problem
					  </ul>
				  <img src="assets/biofeedback.svg" width="35%">
				  <p class="rcred">Lansdell et al IEEE Trans NSRE 2020</p>
				  </div>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  So, this was for 2 neurons. We can test it in a larger network and more interesting setting than an artifical reward function. 
		  
				  [Go through bullet points]
				  We see with RDD-based learning that performance doesn't matter on the correlation of the control neuron with other wrist-controlling neurons. This is similar to empirical findings, whereas learning with the observed dependence, final performance of the network does depend on the amount of correlation.
		  
				  In training, the weights for the individual control unit are more quickly separated from the rest of the units that are doing wrist control, compared to the observed dependence. 
		  
				  Thus this recapitulates findings from BCI learning.
		  
				  • <span style="color: green"></span>
				</aside>
			  </section> -->

			<section data-background-color='#ffffff'>
				<h2>Is this plausible?</h2>
					<ul> <li> It would require:
						<ul>
						<li> sub-threshold dependent plasticity
						<li> neuromodulator dependent plasticity</ul>
					</ul>
				<img src="assets/fig5a.svg" width="55%">
				<p class="rcred">Ngezahayo et al 2000, Seol et al 2007</p>
					<!--<ul class="fragment">
						<li> Additionally would predict super-threshold dependent plasticity
					</ul>-->
		
			  <aside class="notes">
				<span style="color: red"></span> •
				Of course, we can ask if it is realistic for a neuron to do something like RDD-based learning? In fact, there are two components to the model really. The first is the only learning for inputs that plcae the neuron close to threshold. This is in part already established under something called sub-threshold dependent plasticity. Inputs that place a neuron too far below threhsold induce no learning, consistent with RDD. The model would additionally predict that very high super threshold inputs also induce no learning, but this hasn't been explicitly tested yet. 
		
				The other part of the model is the reward dependent component, in which the sign of the update switches with the magnitude of the reward signal. This has been established in some cases, but more experiments are also needed. Here the blue and red curves show the sign of the changes to weights for different neuromodulator concentrations, showing things can turn from potentiating (increasing) to depressing (decreasing). 
		
				Thus both aspects are largely consistent with neurobiology. 
				• <span style="color: green"></span>
			  </aside>
			</section>

			<section data-background-color="#ffffff">
				<h2>Why spike?</h2>
				<ul>
					<li> Neurons need to communicate over large distances
					</ul>
					<div class = "fragment">
					<video class="slideautostart" src="assets/chris_paper_S1.ogv" muted controls loop autoplay
					poster="assets/CorticalFOV.png" width="45%">
					</video>
					<p class="rcred">Calcium imaging in Hydra. Dupre and Yuste 2017 (Video here: benlansdell.github.io/stjude/#/23)</p>
					</div>
					<ul class = "fragment">
					<li> But computationally, a spiking discontinuity is inconvenient for learning
					<li> What are the comptuational benefits of spiking? 
					<li class="fragment"> With RDD-based learning, spiking is a feature and not a bug
				</ul>
				<aside class="notes">
				<span style="color: red"></span> •
				The rule takes the following form. 
		
				We observe the trajectories are less biased and converge faster than when using the observed dependence.
				• <span style="color: green"></span>
			  </aside>
			</section>

			  <section data-background-color="#ffffff">
				  <h2>Summary</h2>
				  <ul>
					<li> RDD can be used to estimate causal effects, and can provide a solution to the credit assignment problem in spiking neural networks
					<li> Shows a neuron can do causal inference without needing to randomize
					<li> Relies on the fact that neurons spike when input exceeds a threshold &ndash; spiking is a feature not a bug
				  </ul>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
			  Thus:
			  * neurons can solve the credit assignment problem without an independent noise source, in the presence of high correlations
			  * can learn with only observation of reward and how close it was to spiking
			  * Spiking is a feature, not a bug!
			  * An answer to why spike?
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  
		  <section data-background-color="#ffffff">
			<h2>Related work</h2>
			<div id="left">
				<ul>
					<li> Using causal effect estimators to train large artificial neural networks
						<p class="lcred">Lansdell et al, 'Learning to solve the credit assignment problem', ICLR 2020 </p>
					<li> Primary motor activity supporting dual-control brain computer interfaces<br>
						<p class="lcred">Lansdell et al, 'Reconfiguring motor circuits for a joint manual and BCI task', IEEE Trans. Neural Systems and Rehabilitation Engineering 2020</p>
					<li> Behavioral analysis and neuron tracking in the cnidarian Hydra<br>
						<p class="lcred">(in progress)</p>
				</ul>
				</div>
				<div id="right">
				<img src="assets/fig1_schematic_np.svg" width="85.5%">
				</div>
		  <aside class="notes">
			<span style="color: red"></span> •
			• <span style="color: green"></span>
		  </aside>
		</section>

		  <section data-background-color="#ffffff">
			<h2>Related work</h2>
			<div id="left">
				<ul>
					<li style="color:red"> Using causal effect estimators to train large artificial neural networks
						<p class="lcred">Lansdell et al, 'Learning to solve the credit assignment problem', ICLR 2020 </p>
					<li> Primary motor activity supporting dual-control brain computer interfaces<br>
						<p class="lcred">Lansdell et al, 'Reconfiguring motor circuits for a joint manual and BCI task', IEEE Trans. Neural Systems and Rehabilitation Engineering 2020</p>
					<li> Behavioral analysis and neuron tracking in the cnidarian Hydra<br>
						<p class="lcred">(in progress)</p>
				</ul>
				</div>
				<div id="right">
				<img src="assets/fig1_schematic_np.svg" width="85.5%">
				</div>
		  <aside class="notes">
			<span style="color: red"></span> •
			• <span style="color: green"></span>
		  </aside>
		</section>

		<section data-background-color="#ffffff">
			<h2>Related work</h2>
			<div id="left">
				<ul>
					<li> Using causal effect estimators to train large artificial neural networks
						<p class="lcred">Lansdell et al, 'Learning to solve the credit assignment problem', ICLR 2020 </p>
					<li style="color:red"> Primary motor activity supporting dual-control brain computer interfaces<br>
						<p class="lcred">Lansdell et al, 'Reconfiguring motor circuits for a joint manual and BCI task', IEEE Trans. Neural Systems and Rehabilitation Engineering 2020</p>
					<li> Behavioral analysis and neuron tracking in the cnidarian Hydra<br>
						<p class="lcred">(in progress)</p>
				</ul>
				</div>
				<div id="right">
				<img src="assets/bci.png" width="80%">
				</div>
		  <aside class="notes">
			<span style="color: red"></span> •
			• <span style="color: green"></span>
		  </aside>
		</section>

		<section data-background-color="#ffffff">
			<h2>Related work</h2>
			<div id="left">
				<ul>
					<li> Using causal effect estimators to train large artificial neural networks
						<p class="lcred">Lansdell et al, 'Learning to solve the credit assignment problem', ICLR 2020 </p>
					<li> Primary motor activity supporting dual-control brain computer interfaces<br>
						<p class="lcred">Lansdell et al, 'Reconfiguring motor circuits for a joint manual and BCI task', IEEE Trans. Neural Systems and Rehabilitation Engineering 2020</p>
					<li style="color:red"> Behavioral analysis and neuron tracking in the cnidarian Hydra<br>
						<p class="lcred">(In progress)</p>
				</ul>
				</div>
				<div id="right">
				<video class="slideautostart" width="90%" src="assets/hydra_trimmed.mp4" muted controls loop autoplay></video>
				<p class="rcred">(Video: benlansdell.github.io/stjude/#/27)</p>
			</div>
		  <aside class="notes">
			<span style="color: red"></span> •
			• <span style="color: green"></span>
		  </aside>
		</section>

		<!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->
		  <!-- ---------------------------------------------------------------------- -->

		  <section data-background-image="./assets/brain2_darkest2.png">
			<h2>References</h2>
				<hr>
			<ul style="font-size: smaller">
				<li> <b>Lansdell B</b>, Kording K, "Spiking allows neurons to estimate their causal effect" biorxiv 2019
				<li> <b>Lansdell B</b>, Prakash P, Kording K, "Learning to solving the credit assignment problem" ICLR 2020
				<li> <b>Lansdell B</b>, Milovanovic I, Mellema C, Fairhall A, Fetz E, Moritz C, "Reconfiguring motor circuits for a joint manual and BCI task" IEEE Trans. Neural Systems and Rehabilitation Engineering, 2020, 28(1)
				 <li><b>Lansdell B</b>, Kording K, "Towards learning-to-learn" Current Opinion in Behavioral Science, 2019, 29, 45-50
				<li>Aljadeff Y, <b>Lansdell B</b>, Fairhall A, Kleinfeld D, "Analysis of neuronal spike trains, deconstructed", Neuron 2016, 91(2)
				<li>Pang R, <b>Lansdell B</b>, Fairhall A, "Dimensionality Reduction in Neuroscience", Current Biology 2016, 26: R1-R5
			</ul>
				<aside class="notes">
			  <span style="color: red"></span> •
			  • <span style="color: green"></span>
			</aside>
		  </section>

		  <section data-background-image="./assets/brain2_darkest2.png">
				<h2>Acknowledgments</h2>
					<hr>
				<div id="left">
			   <ul>
					<li> Konrad Kording (U Penn)
				   <li> Kording lab</li><ul>
					   <li> Ari Benjamin </li>
					   <li> David Rolnick</li>
					   <li> Roozbeh Farhoodi</li>
					   <li> Prashanth Prakash</li></ul>
					<li> Adrienne Fairhall (UW)</li>
					<li> Fairhall lab</li><ul>
						<li> Rich Pang</li>
						<li> Alison Duffy</li>
					</ul>
				</ul>
					</div>
					<div id="right">
					<ul>
					<li> Chet Moritz (UW) </li>
					<li> Ivana Milovanovic (UW)</li>
					<li> Cooper Mellema (UT Austin)</li>
					<li> Eberhard Fetz (UW)</li>
					</ul>
						</div>
					<aside class="notes">
				  <span style="color: red"></span> •
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- ----------------------------------------------------------------------- -->
		  <!-- ----------------------------------------------------------------------- -->
		  
			  <section data-background-color='#ffffff'>
				<h2>RDD as a way for a neuron to solve credit assignment</h2>
				  <img src="assets/rdd_fig1b.svg" width="80%">
				  <p class="rcred">Lansdell and Kording, bioRxiv 2019</p>
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  The idea is that it should only estimate the effect on a reward signal for inputs that place the neuron close to threshold. Over a fixed time window, we have a set of below-threshold inputs, marginally below, marginally above, and above. The idea is that neuron's should only update their estimate of beta for the marginal inputs, and that is a way of isolating that neuron's specific effect on reward. 
				  • <span style="color: green"></span>
				</aside>
			  </section>
		  
			  <section data-background-color='#ffffff'>
				  <h2>How to test?</h2>
					  <ul>
						  <li> Over a fixed time window a reward is administered when neuron spikes
						  <li> Stimuli are identified which place the neuron's input drive close to spiking threshold. 
						  <li> RDD-based learning predicts an increase synaptic changes for a set of stimuli containing a high proportion of near threshold inputs, but that keeps overall firing rate constant.
					  </ul>
				  <img src="assets/fig5b.svg" width="55%">
		  
				<aside class="notes">
				  <span style="color: red"></span> •
				  How would we test RDD-based learning? One idea would be [idea]
				  • <span style="color: green"></span>
				</aside>
			  </section>
			</div>
		</div>

		<script src="js/reveal.js"></script>
		<script src="lib/js/head.min.js"></script>
		<script>
			head.js(
			"lib/js/jquery.min.js",
			"lib/js/jquery.hotkeys.js",
			"lib/js/underscore.min.js",
			"lib/js/swfobject.js",
			"lib/js/dat.gui.js",
			"lib/js/EventEmitter.js",

			function() {
			Reveal.initialize({
			controls: false,
			progress: true,
			history: true,
			center: false,
			keyboard: true,
			touch: false,
			overview: true,
			mouseWheel: false,
			width: 960,
			height: 720,

			theme: false, // hardcoded with CSS import in <head>
			transition: 'fade', // default/cube/page/concave/zoom/linear/fade/none
			transitionSpeed: 'default', // default/fast/slow

			math: {
				mathjax: 'mathjax/MathJax.js',
				config: 'TeX-AMS_HTML-full',
			},

			dependencies: [
				{ src: 'reveal.js/lib/js/classList.js',
			condition: function() { return !document.body.classList; }},
				{ src: 'reveal.js/plugin/markdown/marked.js',
			condition: function() { return !!document.querySelector ('[data-markdown]'); }},
				{ src: 'reveal.js/plugin/markdown/markdown.js',
			condition: function() { return !!document.querySelector ('[data-markdown]'); }},
				{ src: 'reveal.js/plugin/highlight/highlight.js', async: true,
			callback: function() { hljs.initHighlightingOnLoad (); }},
				{ src: 'reveal.js/plugin/notes/notes.js', async: true,
			condition: function() { return !!document.body.classList; }},
				{ src: 'mymath.js', async: true },
			//{ src: 'pdfimgs.js', async: true },
			{ src: 'slideautostart.js', async: true },
			],
			});
		});
  </script>
	</body>
</html>